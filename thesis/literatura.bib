% Topology of neural networks
@article{Gabella2021,
   title={Topology of Learning in Feedforward Neural Networks},
   volume={32},
   ISSN={2162-2388},
   url={http://dx.doi.org/10.1109/TNNLS.2020.3015790},
   DOI={10.1109/tnnls.2020.3015790},
   number={8},
   journal={IEEE Transactions on Neural Networks and Learning Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Gabella, Maxime},
   year={2021},
   month=aug, pages={3588–3592} }

% Backpropagation algorithm
@article{Rumelhart1986,
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden'units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	date = {1986/10/01},
	date-added = {2025-06-26 17:54:57 +0200},
	date-modified = {2025-06-26 17:54:57 +0200},
	doi = {10.1038/323533a0},
	id = {Rumelhart1986},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6088},
	pages = {533--536},
	title = {Learning representations by back-propagating errors},
	url = {https://doi.org/10.1038/323533a0},
	volume = {323},
	year = {1986},
	bdsk-url-1 = {https://doi.org/10.1038/323533a0}}

% Algebraic topology
@book{Munkers84,
  added-at = {2017-06-29T07:13:07.000+0200},
  author = {Munkres, James R.},
  biburl = {https://www.bibsonomy.org/bibtex/2a393fd24fb74ad14a3a577dd049bd850/gdmcbain},
  citeulike-article-id = {12391074},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0201045869},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0201045869},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0201045869},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0201045869},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0201045869/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0201045869},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0201045869},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0201045869},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0201045869\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0201045869},
  howpublished = {Hardcover},
  interhash = {10ba8f346353018af3b6905ac8a49e37},
  intrahash = {a393fd24fb74ad14a3a577dd049bd850},
  isbn = {0201045869},
  keywords = {57-01-manifolds-cell-complexes-instructional-exposition 55-01-algebraic-topology-instructional-exposition},
  posted-at = {2015-03-03 06:47:23},
  priority = {2},
  publisher = {Addison Wesley Publishing Company},
  timestamp = {2019-03-25T05:31:34.000+0100},
  title = {{Elements of Algebraic Topology}},
  url = {http://www.worldcat.org/isbn/0201045869},
  year = 1984
}

% Elements of statistical learning
@book{Hastie2009,
  added-at = {2018-12-03T19:42:16.000+0100},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/218c90decd17003e5a43ca50b03f1f1d1/tobydriscoll},
  doi = {10.1007/978-0-387-84858-7},
  file = {:Book/Hastie2009 - The Elements of Statistical Learning.pdf:PDF},
  interhash = {39bdf0a76d889fa39deb5d1ac793ff4e},
  intrahash = {18c90decd17003e5a43ca50b03f1f1d1},
  keywords = {imported},
  publisher = {Springer New York},
  timestamp = {2018-12-03T19:43:16.000+0100},
  title = {The Elements of Statistical Learning},
  year = 2009
}

% TODO
% Methods for interpreting and understanding neural networks
@article{MONTAVON20181,
title = {Methods for interpreting and understanding deep neural networks},
journal = {Digital Signal Processing},
volume = {73},
pages = {1-15},
year = {2018},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2017.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S1051200417302385},
author = {Grégoire Montavon and Wojciech Samek and Klaus-Robert Müller},
keywords = {Deep neural networks, Activation maximization, Sensitivity analysis, Taylor decomposition, Layer-wise relevance propagation},
abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.}
}

% PCA
@misc{shlens2014tutorial,
      title={A Tutorial on Principal Component Analysis}, 
      author={Jonathon Shlens},
      year={2014},
      eprint={1404.1100},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Mapper
@misc{Langenbahn2022,
  author = {Langenbahn, Gretchen},
  title = {Topological Data Analysis with Mapper},
  year = {2022},
  howpublished = {\url{https://scholarworks.bgsu.edu/honorsprojects/732}},
  note = {Honors Projects. 732},
  school = {Bowling Green State University},
  department = {Mathematics and Statistics; Computer Science},
  advisor = {Dr. Umar Islambekov; Dr. Jong Kwan “Jake” Lee},
  publicationdate = {Spring 4-27-2022},
  major = {Mathematics}
}

% Diplomsko delo nevronske mreže
@phdthesis{GuzeljBlatnik2020,
  author = {Guzelj Blatnik, Laura},
  title = {Nevronske mreže z vzvratnim razširjanjem napak v funkcijskem programskem jeziku: delo diplomskega seminarja},
  school = {Diplomsko delo},
  year = {2020},
  note = {Dostopano 31 marec 2024},
  url = {https://repozitorij.uni-lj.si/IzpisGradiva.php?lang=slv&id=117661}
}

% Neural networks state of art, brief history, basic models and arhitecture
@InProceedings{10.1007/978-3-319-45378-1_1,
author="Macukow, Bohdan",
editor="Saeed, Khalid
and Homenda, W{\l}adys{\l}aw",
title="Neural Networks -- State of Art, Brief History, Basic Models and Architecture",
booktitle="Computer Information Systems and Industrial Management",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="3--14",
abstract="The history of neural networks can be traced back to the work of trying to model the neuron. Today, neural networks discussions are occurring everywhere. Neural networks, with their remarkable ability to derive meaning from complicated or imprecise data, can be used to extract patterns and detect trends that are too complex to be noticed by either humans or other computer techniques. A brief history of the neural networks research is presented and some more popular models are briefly discussed. The major attention is on the feed-forward networks and specially to the topology of such the network and method of building the multi-layer perceptrons.",
isbn="978-3-319-45378-1"
}

% Backpropagation
@article{rojas1996backpropagation,
  title={The backpropagation algorithm},
  author={Rojas, Raul and Rojas, Ra{\'u}l},
  journal={Neural networks: a systematic introduction},
  pages={149--182},
  year={1996},
  publisher={Springer}
}

% Introduction to TDA
@misc{chazal2021introductiontopologicaldataanalysis,
      title={An introduction to Topological Data Analysis: fundamental and practical aspects for data scientists}, 
      author={Frédéric Chazal and Bertrand Michel},
      year={2021},
      eprint={1710.04019},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/1710.04019}, 
}

@misc{zupan2024uozp,
  author = {Blaz Zupan},
  title = {Uvod v Odkrivanje Znanj iz Podatkov},
  year = {2024},
  howpublished = {\url{https://github.com/BlazZupan/uozp-zapiski/blob/master/zapiski.pdf}},
  note = {Accessed: 2024-07-31}
}
@misc{gormley2018pca,
  author = {Matthew Gormley},
  title = {Lecture 11: Principal Component Analysis (PCA)},
  year = {2018},
  howpublished = {\url{http://www.cs.cmu.edu/~mgormley/courses/606-607-f18/slides606/lecture11-pca.pdf}},
  note = {Accessed: 2024-07-31}
}

@misc{kun2015cech,
  author = {Jeremy Kun},
  title = {Čech, Vietoris-Rips, and Dowker Nerves},
  year = {2015},
  howpublished = {\url{https://www.jeremykun.com/2015/08/06/cech-vietoris-rips-complex/}},
  note = {Accessed: 2024-07-31}
}

@misc{researchgate_simplicial_complex,
  author = {Unknown},
  title = {Simplicial Complex: An example of a simplicial complex composed of eight vertices},
  year = {2022},
  howpublished = {\url{https://www.researchgate.net/publication/358508552/figure/fig4/AS:1134810642817026@1647571349635/Simplicial-complex-An-example-of-a-simplicial-complex-composed-of-eight-vertices.png}},
  note = {Accessed: 2024-07-31}
}


@phdthesis{Urbančič_2020, title={Aproksimacija mnogoterosti z mehkimi simplicialnimi množicami in njena implementacija v algoritmu UMAP}, url={https://repozitorij.uni-lj.si/IzpisGradiva.php?lang=slv&id=120124}, abstractNote={Motivacija zaključnega dela izvira iz algoritma UMAP (ang. „Uniform Manifold Approximation and Projection“) za zmanjševanje dimenzij, ki so ga leta 2018 v svojem članku predstavili L. McInnes, J. Healy in J. Melville. Obravnavali bomo njegovo interpretacijo kot poseben primer uporabe mehkih simplicialnih množic za aproksimacijo mnogoterosti, ki ga ločuje od drugih metod s področja učenja mnogoterosti. Do definicije mehkih simplicialnih množic bomo prišli s postopnim posploševanjem pojma simplicialnega kompleksa, pri čemer bomo vseskozi uporabljali jezik teorije kategorij. Aproksimacijo mnogoterosti podatkov bomo opisali s posplošitvijo funktorjev singularne množice in geometrijske realizacije za kategorijo omejenih mehkih simplicialnih množic ${cal F}in$-$s{cal F}uzz$ in kategorijo končnih razširjenih psevdometričnih prostorov ${cal F}in{cal EPM}et$ ter predstavili njeno implementacijo v algoritmu UMAP.}, author={Urbančič, Živa}, year={2020}}

@misc{schneider_simplexes,
  author = {Jesse Schneider},
  title = {Simplexes},
  year = {2024},
  howpublished = {\url{http://homepages.math.uic.edu/~jschnei3/Writing/Simplexes}},
  note = {Accessed: 2024-07-31}
}


@Article{electronics10212689,
AUTHOR = {Abdolrasol, Maher G. M. and Hussain, S. M. Suhail and Ustun, Taha Selim and Sarker, Mahidur R. and Hannan, Mahammad A. and Mohamed, Ramizi and Ali, Jamal Abd and Mekhilef, Saad and Milad, Abdalrhman},
TITLE = {Artificial Neural Networks Based Optimization Techniques: A Review},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {2689},
URL = {https://www.mdpi.com/2079-9292/10/21/2689},
ISSN = {2079-9292},
ABSTRACT = {In the last few years, intensive research has been done to enhance artificial intelligence (AI) using optimization techniques. In this paper, we present an extensive review of artificial neural networks (ANNs) based optimization algorithm techniques with some of the famous optimization techniques, e.g., genetic algorithm (GA), particle swarm optimization (PSO), artificial bee colony (ABC), and backtracking search algorithm (BSA) and some modern developed techniques, e.g., the lightning search algorithm (LSA) and whale optimization algorithm (WOA), and many more. The entire set of such techniques is classified as algorithms based on a population where the initial population is randomly created. Input parameters are initialized within the specified range, and they can provide optimal solutions. This paper emphasizes enhancing the neural network via optimization algorithms by manipulating its tuned parameters or training parameters to obtain the best structure network pattern to dissolve the problems in the best way. This paper includes some results for improving the ANN performance by PSO, GA, ABC, and BSA optimization techniques, respectively, to search for optimal parameters, e.g., the number of neurons in the hidden layers and learning rate. The obtained neural net is used for solving energy management problems in the virtual power plant system.},
DOI = {10.3390/electronics10212689}
}

@misc{venkatesh2021role,
  author = {Sai Venkatesh},
  title = {The Role of CSPs in Grabbing Opportunities for Survival},
  year = {2021},
  howpublished = {\url{https://www.linkedin.com/pulse/role-csps-grabbing-opportunities-survival-sai-venkatesh/}},
  note = {Accessed: 2024-07-31}
}

@dataset{dataset,
author = {Ali, Humayra and Powers, David},
year = {2014},
month = {09},
pages = {},
title = {Published DICTA Humayra}
}

@misc{quora_pca_explanation,
  author = {Amit Khumar},
  title = {What is an intuitive explanation for PCA?},
  year = {2024},
  howpublished = {\url{https://www.quora.com/What-is-an-intuitive-explanation-for-PCA}},
  note = {Accessed: 2024-07-31}
}

@misc{justinmath_persistent_homology,
  author = {Justin Skycak},
  title = {Intuiting Persistent Homology},
  year = {2023},
  howpublished = {\url{https://www.justinmath.com/intuiting-persistent-homology/}},
  note = {Accessed: 2024-07-31}
}

